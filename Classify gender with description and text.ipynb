{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "\n",
    "def unzip(zip_filepath, dest_path):\n",
    "    \"\"\"\n",
    "        解压zip文件\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_filepath) as zf:\n",
    "        zf.extractall(path=dest_path)\n",
    "\n",
    "\n",
    "def get_dataset_filename(zip_filepath):\n",
    "    \"\"\"\n",
    "        获取数据库文件名\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_filepath) as zf:\n",
    "        return zf.namelist()[0]\n",
    "\n",
    "\n",
    "def cal_acc(true_labels, pred_labels):\n",
    "    \"\"\"\n",
    "        计算准确率\n",
    "    \"\"\"\n",
    "    n_total = len(true_labels)\n",
    "    correct_list = [true_labels[i] == pred_labels[i] for i in range(n_total)]\n",
    "\n",
    "    acc = sum(correct_list) / n_total\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from skimage import exposure, img_as_float\n",
    "\n",
    "\n",
    "# 头像图片保存路径\n",
    "profile_image_path = './pro_img/'\n",
    "\n",
    "\n",
    "def inspect_dataset(df_data):\n",
    "    \"\"\"pytoho\n",
    "        查看加载的数据基本信息\n",
    "    \"\"\"\n",
    "    print('数据集基本信息：')\n",
    "    print(df_data.info())\n",
    "    print('数据集有{}行，{}列'.format(df_data.shape[0], df_data.shape[1]))\n",
    "    print('数据预览:')\n",
    "    print(df_data.head())\n",
    "\n",
    "\n",
    "def check_profile_image(img_link):\n",
    "    \"\"\"\n",
    "        判断头像图片链接是否有效\n",
    "        如果有效，下载到本地，并且返回保存路径\n",
    "    \"\"\"\n",
    "    save_image_path = ''\n",
    "    # 有效的图片扩展名\n",
    "    valid_img_ext_lst = ['.jpeg', '.png', '.jpg']\n",
    "\n",
    "    try:\n",
    "        img_data = io.imread(img_link)#读取图像\n",
    "        image_name = img_link.rsplit('/')[-1]# rsplit() 方法通过指定分隔符对字符串进行分割并返回一个列表\n",
    "        if any(valid_img_ext in image_name.lower() for valid_img_ext in valid_img_ext_lst):\n",
    "            # 确保图片文件包含有效的扩展名\n",
    "            save_image_path = os.path.join(profile_image_path, image_name)\n",
    "            io.imsave(save_image_path, img_data)\n",
    "    except:\n",
    "        print('头像链接 {} 无效'.format(img_link))\n",
    "        #{} {}\".format(\"hello\", \"world\")   不设置指定位置，按默认顺序 'hello world'\n",
    "\n",
    "\n",
    "    return save_image_path\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        清洗文本数据\n",
    "    \"\"\"\n",
    "    # just in case\n",
    "    text = text.lower()\n",
    "\n",
    "    # 去除特殊字符\n",
    "    text = re.sub('\\s\\W', ' ', text)\n",
    "    text = re.sub('\\W\\s', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def split_train_test(df_data, size=0.8):\n",
    "    \"\"\"\n",
    "        分割训练集和测试集\n",
    "    \"\"\"\n",
    "    # 为保证每个类中的数据能在训练集中和测试集中的比例相同，所以需要依次对每个类进行处理\n",
    "    df_train = pd.DataFrame()\n",
    "    df_test = pd.DataFrame()\n",
    "\n",
    "    labels = [0, 1]\n",
    "    for label in labels:\n",
    "        # 找出gender的记录\n",
    "        text_df_w_label = df_data[df_data['label'] == label]\n",
    "        # 重新设置索引，保证每个类的记录是从0开始索引，方便之后的拆分\n",
    "        text_df_w_label = text_df_w_label.reset_index()\n",
    "\n",
    "        # 默认按80%训练集，20%测试集分割\n",
    "        # 这里为了简化操作，取前80%放到训练集中，后20%放到测试集中\n",
    "        # 当然也可以随机拆分80%，20%（尝试实现下DataFrame中的随机拆分）\n",
    "\n",
    "        # 该类数据的行数\n",
    "        n_lines = text_df_w_label.shape[0]\n",
    "        split_line_no = math.floor(n_lines * size)\n",
    "        text_df_w_label_train = text_df_w_label.iloc[:split_line_no, :]\n",
    "        text_df_w_label_test = text_df_w_label.iloc[split_line_no:, :]\n",
    "\n",
    "        # 放入整体训练集，测试集中\n",
    "        df_train = df_train.append(text_df_w_label_train)\n",
    "        df_test = df_test.append(text_df_w_label_test)\n",
    "\n",
    "    df_train = df_train.reset_index()\n",
    "    df_test = df_test.reset_index()\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "def get_word_list_from_data(text_s):\n",
    "    \"\"\"\n",
    "        将数据集中的单词放入到一个列表中\n",
    "    \"\"\"\n",
    "    word_list = []\n",
    "    for _, text in text_s.iteritems():\n",
    "        word_list += text.split(' ')\n",
    "    return word_list\n",
    "\n",
    "\n",
    "def proc_text(text):\n",
    "    \"\"\"\n",
    "        分词+去除停用词\n",
    "    \"\"\"\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    words = tokenizer.tokenize(text)\n",
    "    filtered_words = [word for word in words if word not in stopwords.words('english')]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "\n",
    "def extract_tf_idf(text_s, text_collection, common_words_freqs):\n",
    "    \"\"\"\n",
    "        提取tf-idf特征\n",
    "    \"\"\"\n",
    "    # 这里只选择TF-IDF特征作为例子\n",
    "    # 可考虑使用词频或其他文本特征作为额外的特征\n",
    "\n",
    "    n_sample = text_s.shape[0]\n",
    "    n_feat = len(common_words_freqs)\n",
    "\n",
    "    common_words = [word for word, _ in common_words_freqs]\n",
    "\n",
    "    # 初始化\n",
    "    X = np.zeros([n_sample, n_feat])\n",
    "\n",
    "    print('提取tf-idf特征...')\n",
    "    for i, text in text_s.iteritems():\n",
    "        feat_vec = []\n",
    "        for word in common_words:\n",
    "            if word in text:\n",
    "                # 如果在高频词中，计算TF-IDF值\n",
    "                tf_idf_val = text_collection.tf_idf(word, text)\n",
    "            else:\n",
    "                tf_idf_val = 0\n",
    "\n",
    "            feat_vec.append(tf_idf_val)\n",
    "\n",
    "        # 赋值\n",
    "        X[i, :] = np.array(feat_vec)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def hex_to_rgb(value):\n",
    "    \"\"\"\n",
    "        十六进制颜色码转换为RGB值\n",
    "    \"\"\"\n",
    "    rgb_list = list(int(value[i:i + 2], 16) for i in range(0, 6, 2))\n",
    "    return rgb_list\n",
    "\n",
    "\n",
    "def extract_rgb_feat(hex_color_s):\n",
    "    \"\"\"\n",
    "         从十六进制颜色码中提取RGB值作为特征\n",
    "    \"\"\"\n",
    "    n_sample = hex_color_s.shape[0]\n",
    "    n_feat = 3\n",
    "\n",
    "    # 初始化\n",
    "    X = np.zeros([n_sample, n_feat])\n",
    "\n",
    "    print('提取RGB特征...')\n",
    "    for i, hex_val in hex_color_s.iteritems():\n",
    "        feat_vec = hex_to_rgb(hex_val)\n",
    "\n",
    "        # 赋值\n",
    "        X[i, :] = np.array(feat_vec)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def extract_rgb_hist_feat(img_path_s):\n",
    "    \"\"\"\n",
    "        从图像中提取RGB直方图特征\n",
    "    \"\"\"\n",
    "    n_sample = img_path_s.shape[0]\n",
    "    n_bins = 100    # 每个通道bin的个数\n",
    "    n_feat = n_bins * 3\n",
    "\n",
    "    # 初始化\n",
    "    X = np.zeros([n_sample, n_feat])\n",
    "\n",
    "    print('提取RGB直方图特征...')\n",
    "    for i, img_path in img_path_s.iteritems():\n",
    "        # 加载图像\n",
    "        img_data = io.imread(img_path)\n",
    "        img_data = img_as_float(img_data)\n",
    "\n",
    "        if img_data.ndim == 3:\n",
    "            # 3个通道\n",
    "            hist_r, _ = exposure.histogram(img_data[:, :, 0], nbins=n_bins)\n",
    "            hist_g, _ = exposure.histogram(img_data[:, :, 1], nbins=n_bins)\n",
    "            hist_b, _ = exposure.histogram(img_data[:, :, 2], nbins=n_bins)\n",
    "        else:\n",
    "            # 2个通道\n",
    "            hist, _ = exposure.histogram(img_data, nbins=n_bins)\n",
    "            hist_r = hist.copy()\n",
    "            hist_g = hist.copy()\n",
    "            hist_b = hist.copy()\n",
    "\n",
    "        feat_vec = np.concatenate((hist_r, hist_b, hist_g))\n",
    "\n",
    "        # 赋值\n",
    "        X[i, :] = np.array(feat_vec)\n",
    "\n",
    "    return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from common_tools import get_dataset_filename, unzip, cal_acc\n",
    "from pd_tools import inspect_dataset, check_profile_image, \\\n",
    "    split_train_test, clean_text, proc_text, get_word_list_from_data, \\\n",
    "    extract_tf_idf, extract_rgb_feat, extract_rgb_hist_feat\n",
    "import nltk\n",
    "from nltk.text import TextCollection\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "# 声明数据集路径\n",
    "dataset_path = './dataset'  # 数据集路径\n",
    "zip_filename = 'twitter-user-gender-classification.zip'  # zip文件名\n",
    "zip_filepath = os.path.join(dataset_path, zip_filename)  # zip文件路径\n",
    "cln_datapath = './cln_data.csv'     # 清洗好的数据路径\n",
    "\n",
    "# 是否第一次运行\n",
    "is_first_run = True\n",
    "\n",
    "\n",
    "def run_main():\n",
    "    \"\"\"\n",
    "        主函数\n",
    "    \"\"\"\n",
    "    # 声明变量\n",
    "    dataset_filename = get_dataset_filename(zip_filepath)  # 数据集文件名（在zip中）\n",
    "    dataset_filepath = os.path.join(dataset_path, dataset_filename)  # 数据集文件路径\n",
    "\n",
    "    if is_first_run:\n",
    "\n",
    "        print('解压zip...', end='')\n",
    "        unzip(zip_filepath, dataset_path)\n",
    "        print('完成.')\n",
    "\n",
    "        # 读取数据\n",
    "        data = pd.read_csv(dataset_filepath, encoding='latin1',\n",
    "                           usecols=['gender', 'description', 'link_color',\n",
    "                                    'profileimage', 'sidebar_color', 'text'])\n",
    "        # 1. 查看加载的数据集\n",
    "        inspect_dataset(data)\n",
    "\n",
    "        # 2. 数据清洗\n",
    "        # 2.1. 根据 'gender' 列过滤数据\n",
    "        filtered_data = data[(data['gender'] == 'male') | (data['gender'] == 'female')]\n",
    "\n",
    "        # 2.2 过滤掉 'description' 列为空的数据\n",
    "        filtered_data = filtered_data.dropna(subset=['description'])\n",
    "\n",
    "        # 2.3 过滤掉 'link_color' 列和 'sidebar_color' 列非法的16进制数据\n",
    "        filtered_data = filtered_data[filtered_data['link_color'].str.len() == 6]\n",
    "        filtered_data = filtered_data[filtered_data['sidebar_color'].str.len() == 6]\n",
    "\n",
    "        # 2.4 清洗文本数据\n",
    "        print('清洗文本数据...')\n",
    "        cln_desc = filtered_data['description'].apply(clean_text)\n",
    "        cln_text = filtered_data['text'].apply(clean_text)\n",
    "        filtered_data['cln_desc'] = cln_desc\n",
    "        filtered_data['cln_text'] = cln_text\n",
    "\n",
    "        # 2.5 根据profileimage的链接判断头像图片是否有效，\n",
    "        # 并生成新的列代表头像图片保存的路径\n",
    "        print('下载头像数据...')\n",
    "        saved_img_s = filtered_data['profileimage'].apply(check_profile_image)\n",
    "        filtered_data['saved_image'] = saved_img_s\n",
    "        # 过滤掉无效的头像数据\n",
    "        filtered_data = filtered_data[filtered_data['saved_image'] != '']\n",
    "\n",
    "        # 保存处理好的数据\n",
    "        filtered_data.to_csv(cln_datapath, index=False)\n",
    "\n",
    "    # 读取处理好的数据\n",
    "    clean_data = pd.read_csv(cln_datapath, encoding='latin1',\n",
    "                             usecols=['gender', 'cln_desc', 'cln_text',\n",
    "                                      'link_color', 'sidebar_color', 'saved_image'])\n",
    "\n",
    "    # 查看label的分布\n",
    "    print(clean_data.groupby('gender').size())\n",
    "\n",
    "    # 替换male->0, female->1\n",
    "    clean_data.loc[clean_data['gender'] == 'male', 'label'] = 0\n",
    "    clean_data.loc[clean_data['gender'] == 'female', 'label'] = 1\n",
    "\n",
    "    # 3. 分割数据集\n",
    "    # 分词 去除停用词\n",
    "    proc_desc_s = clean_data['cln_desc'].apply(proc_text)\n",
    "    clean_data['desc_words'] = proc_desc_s\n",
    "\n",
    "    proc_text_s = clean_data['cln_text'].apply(proc_text)\n",
    "    clean_data['text_words'] = proc_text_s\n",
    "\n",
    "    df_train, df_test = split_train_test(clean_data)\n",
    "    # 查看训练集测试集基本信息\n",
    "    print('训练集中各类的数据个数：', df_train.groupby('label').size())\n",
    "    print('测试集中各类的数据个数：', df_test.groupby('label').size())\n",
    "\n",
    "    # 4. 特征工程\n",
    "    # 4.1 训练数据特征提取\n",
    "    print('训练样本特征提取：')\n",
    "    # 4.1.1 文本数据\n",
    "    # description数据\n",
    "    print('统计description词频...')\n",
    "    n_desc_common_words = 50\n",
    "    desc_words_in_train = get_word_list_from_data(df_train['desc_words'])\n",
    "    fdisk = nltk.FreqDist(desc_words_in_train)\n",
    "    desc_common_words_freqs = fdisk.most_common(n_desc_common_words)\n",
    "    print('descriptino中出现最多的{}个词是：'.format(n_desc_common_words))\n",
    "    for word, count in desc_common_words_freqs:\n",
    "        print('{}: {}次'.format(word, count))\n",
    "    print()\n",
    "\n",
    "    # 提取desc文本的TF-IDF特征\n",
    "    print('提取desc文本特征...', end=' ')\n",
    "    desc_collection = TextCollection(df_train['desc_words'].values.tolist())\n",
    "    tr_desc_feat = extract_tf_idf(df_train['desc_words'], desc_collection, desc_common_words_freqs)\n",
    "    print('完成')\n",
    "    print()\n",
    "\n",
    "    # text数据\n",
    "    print('统计text词频...')\n",
    "    n_text_common_words = 50\n",
    "    text_words_in_train = get_word_list_from_data(df_train['text_words'])\n",
    "    fdisk = nltk.FreqDist(text_words_in_train)\n",
    "    text_common_words_freqs = fdisk.most_common(n_text_common_words)\n",
    "    print('text中出现最多的{}个词是：'.format(n_text_common_words))\n",
    "    for word, count in text_common_words_freqs:\n",
    "        print('{}: {}次'.format(word, count))\n",
    "    print()\n",
    "\n",
    "    # 提取text文本TF-IDF特征\n",
    "    text_collection = TextCollection(df_train['text_words'].values.tolist())\n",
    "    print('提取text文本特征...', end=' ')\n",
    "    tr_text_feat = extract_tf_idf(df_train['text_words'], text_collection, text_common_words_freqs)\n",
    "    print('完成')\n",
    "    print()\n",
    "\n",
    "    # 4.1.2 图像数据\n",
    "    # link color的RGB特征\n",
    "    tr_link_color_feat_ = extract_rgb_feat(df_train['link_color'])\n",
    "    tr_sidebar_color_feat = extract_rgb_feat(df_train['sidebar_color'])\n",
    "\n",
    "    # 头像的RGB直方图特征\n",
    "    tr_profile_img_hist_feat = extract_rgb_hist_feat(df_train['saved_image'])\n",
    "\n",
    "    # 组合文本特征和图像特征\n",
    "    tr_feat = np.hstack((tr_desc_feat, tr_text_feat, tr_link_color_feat_,\n",
    "                         tr_sidebar_color_feat, tr_profile_img_hist_feat))\n",
    "\n",
    "    # 特征范围归一化\n",
    "    scaler = StandardScaler()\n",
    "    tr_feat_scaled = scaler.fit_transform(tr_feat)\n",
    "\n",
    "    # 获取训练集标签\n",
    "    tr_labels = df_train['label'].values\n",
    "\n",
    "    # 4.2 测试数据特征提取\n",
    "    print('测试样本特征提取：')\n",
    "    # 4.2.1 文本数据\n",
    "    # description数据\n",
    "    # 提取desc文本的TF-IDF特征\n",
    "    print('提取desc文本特征...', end=' ')\n",
    "    te_desc_feat = extract_tf_idf(df_test['desc_words'], desc_collection, desc_common_words_freqs)\n",
    "    print('完成')\n",
    "    print()\n",
    "\n",
    "    # text数据\n",
    "    # 提取text文本TF-IDF特征\n",
    "    print('提取text文本特征...', end=' ')\n",
    "    te_text_feat = extract_tf_idf(df_test['text_words'], text_collection, text_common_words_freqs)\n",
    "    print('完成')\n",
    "    print()\n",
    "\n",
    "    # 4.2.2 图像数据\n",
    "    # link color的RGB特征\n",
    "    te_link_color_feat_ = extract_rgb_feat(df_test['link_color'])\n",
    "    te_sidebar_color_feat = extract_rgb_feat(df_test['sidebar_color'])\n",
    "\n",
    "    # 头像的RGB直方图特征\n",
    "    te_profile_img_hist_feat = extract_rgb_hist_feat(df_test['saved_image'])\n",
    "\n",
    "    # 组合文本特征和图像特征\n",
    "    te_feat = np.hstack((te_desc_feat, te_text_feat, te_link_color_feat_,\n",
    "                         te_sidebar_color_feat, te_profile_img_hist_feat))\n",
    "\n",
    "    # 特征范围归一化\n",
    "    te_feat_scaled = scaler.transform(te_feat)\n",
    "\n",
    "    # 获取训练集标签\n",
    "    te_labels = df_test['label'].values\n",
    "\n",
    "    # 4.3 PCA降维操作\n",
    "    pca = PCA(n_components=0.95)  # 保留95%累计贡献率的特征向量\n",
    "    tr_feat_scaled_pca = pca.fit_transform(tr_feat_scaled)\n",
    "    te_feat_scaled_pca = pca.transform(te_feat_scaled)\n",
    "\n",
    "    # 5. 模型建立训练，对比PCA操作前后的效果\n",
    "    # 使用未进行PCA操作的特征\n",
    "    lr_model = LogisticRegression()\n",
    "    lr_model.fit(tr_feat_scaled, tr_labels)\n",
    "\n",
    "    # 使用PCA操作后的特征\n",
    "    lr_pca_model = LogisticRegression()\n",
    "    lr_pca_model.fit(tr_feat_scaled_pca, tr_labels)\n",
    "\n",
    "    # 6. 模型测试\n",
    "    pred_labels = lr_model.predict(te_feat_scaled)\n",
    "    pred_pca_labels = lr_pca_model.predict(te_feat_scaled_pca)\n",
    "    # 准确率\n",
    "    print('未进行PCA操作:')\n",
    "    print('样本维度：', tr_feat_scaled.shape[1])\n",
    "    print('准确率：{}'.format(cal_acc(te_labels, pred_labels)))\n",
    "\n",
    "    print()\n",
    "    print('进行PCA操作后:')\n",
    "    print('样本维度：', tr_feat_scaled_pca.shape[1])\n",
    "    print('准确率：{}'.format(cal_acc(te_labels, pred_pca_labels)))\n",
    "\n",
    "    # 7. 删除解压数据，清理空间\n",
    "    if os.path.exists(dataset_filepath):\n",
    "        os.remove(dataset_filepath)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
